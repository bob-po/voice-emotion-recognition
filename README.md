# ğŸ¤ ä¸­æ–‡è¯­éŸ³æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ

åŸºäºHugging Faceä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹çš„è¯­éŸ³æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿï¼Œæ”¯æŒ6ç§æƒ…ç»ªåˆ†ç±»ï¼šæ„¤æ€’ã€ææƒ§ã€å¿«ä¹ã€æ‚²ä¼¤ã€æƒŠè®¶å’Œä¸­æ€§ã€‚

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- ğŸš€ **é¢„è®­ç»ƒæ¨¡å‹**: ä½¿ç”¨Hugging Faceä¸Šçš„é«˜è´¨é‡ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹
- ğŸ¯ **6ç§æƒ…ç»ª**: æ”¯æŒæ„¤æ€’ã€ææƒ§ã€å¿«ä¹ã€æ‚²ä¼¤ã€æƒŠè®¶ã€ä¸­æ€§
- ğŸ“ **å¤šç§è¾“å…¥**: æ”¯æŒéŸ³é¢‘æ–‡ä»¶ä¸Šä¼ å’Œå®æ—¶å½•éŸ³
- ğŸŒ **Webç•Œé¢**: æä¾›å‹å¥½çš„Gradio Webç•Œé¢
- ğŸ’» **å‘½ä»¤è¡Œå·¥å…·**: æ”¯æŒæ‰¹é‡å¤„ç†å’Œè„šæœ¬è°ƒç”¨
- ğŸ“Š **å¯è§†åŒ–**: æä¾›æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒå›¾è¡¨
- ğŸ”§ **è‡ªåŠ¨é¢„å¤„ç†**: è‡ªåŠ¨éŸ³é¢‘æ ¼å¼è½¬æ¢å’Œé‡é‡‡æ ·
- ğŸ¨ **ä¸­æ–‡ç•Œé¢**: å®Œå…¨ä¸­æ–‡åŒ–çš„ç”¨æˆ·ç•Œé¢

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- CUDAæ”¯æŒï¼ˆå¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿï¼‰
- è‡³å°‘4GBå†…å­˜
- ç½‘ç»œè¿æ¥ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰

## ğŸ› ï¸ å®‰è£…æ­¥éª¤

### 1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/example/voice-emotion-recognition.git
cd voice-emotion-recognition
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
```bash
# ä½¿ç”¨conda
conda create -n voice-emotion python=3.9
conda activate voice-emotion

# æˆ–ä½¿ç”¨venv
python -m venv voice-emotion
source voice-emotion/bin/activate  # Linux/Mac
# voice-emotion\Scripts\activate  # Windows
```

### 3. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 4. éªŒè¯å®‰è£…
```bash
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')"
python -c "import transformers; print(f'Transformersç‰ˆæœ¬: {transformers.__version__}')"
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: Webç•Œé¢ï¼ˆæ¨èï¼‰

å¯åŠ¨Webç•Œé¢ï¼š
```bash
python web_interface.py
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ `http://localhost:7860`

**ä½¿ç”¨æ­¥éª¤ï¼š**
1. ç‚¹å‡»"ğŸš€ åŠ è½½æ¨¡å‹"æŒ‰é’®
2. é€‰æ‹©"ğŸ“ ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"æˆ–"ğŸ¤ å®æ—¶å½•éŸ³"æ ‡ç­¾é¡µ
3. ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–å½•åˆ¶éŸ³é¢‘
4. ç‚¹å‡»"ğŸ” åˆ†æ"æŒ‰é’®
5. æŸ¥çœ‹é¢„æµ‹ç»“æœå’Œæƒ…ç»ªåˆ†å¸ƒå›¾

### æ–¹æ³•2: å‘½ä»¤è¡Œå·¥å…·

#### åˆ†æå•ä¸ªéŸ³é¢‘æ–‡ä»¶
```bash
python cli_tool.py -f audio.wav
```

#### æ‰¹é‡åˆ†æå¤šä¸ªéŸ³é¢‘æ–‡ä»¶
```bash
python cli_tool.py -f audio1.wav audio2.wav audio3.wav
```

#### è¯¦ç»†è¾“å‡ºæ¨¡å¼
```bash
python cli_tool.py -f audio.wav -v
```

#### ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
```bash
python cli_tool.py -f audio.wav -o result.json
```

### æ–¹æ³•3: Python API

```python
from emotion_speech_recognition import EmotionSpeechRecognition

# åˆå§‹åŒ–æ¨¡å‹
recognizer = EmotionSpeechRecognition()

# é¢„æµ‹éŸ³é¢‘æ–‡ä»¶
result = recognizer.predict_emotion("audio.wav")
print(f"é¢„æµ‹æƒ…ç»ª: {result['predicted_emotion']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")

# ä»éŸ³é¢‘æ•°ç»„é¢„æµ‹
import numpy as np
audio_array = np.random.randn(16000)  # ç¤ºä¾‹éŸ³é¢‘æ•°æ®
result = recognizer.predict_emotion_from_array(audio_array, sample_rate=16000)
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
voice-emotion-recognition/
â”œâ”€â”€ requirements.txt              # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ emotion_speech_recognition.py # æ ¸å¿ƒè¯†åˆ«ç±»
â”œâ”€â”€ web_interface.py             # Webç•Œé¢
â”œâ”€â”€ cli_tool.py                  # å‘½ä»¤è¡Œå·¥å…·
â”œâ”€â”€ run.py                       # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_chinese_model.py        # ä¸­æ–‡æ¨¡å‹æµ‹è¯•
â”œâ”€â”€ test_system.py               # ç³»ç»Ÿæµ‹è¯•
â”œâ”€â”€ emotion_chart.png            # ç¤ºä¾‹å›¾è¡¨
â””â”€â”€ README.md                    # é¡¹ç›®è¯´æ˜
```

## ğŸ¯ æ”¯æŒçš„éŸ³é¢‘æ ¼å¼

- **è¾“å…¥æ ¼å¼**: WAV, MP3, M4A, FLAC, OGGç­‰
- **é‡‡æ ·ç‡**: è‡ªåŠ¨é‡é‡‡æ ·åˆ°16kHz
- **å£°é“**: è‡ªåŠ¨è½¬æ¢ä¸ºå•å£°é“
- **æ—¶é•¿**: å»ºè®®3-10ç§’ï¼Œæ”¯æŒæ›´é•¿éŸ³é¢‘

## ğŸ”§ æ¨¡å‹ä¿¡æ¯

- **æ¨¡å‹åç§°**: `xmj2002/hubert-base-ch-speech-emotion-recognition`
- **æ¶æ„**: Hubert + Transformer
- **è®­ç»ƒæ•°æ®**: CASIAä¸­æ–‡è¯­éŸ³æ•°æ®é›†
- **æƒ…ç»ªç±»åˆ«**: 6ç§ï¼ˆangry, fear, happy, sad, surprise, neutralï¼‰
- **è¯­è¨€**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­éŸ³ä¼˜åŒ–

## ğŸ“Š è¾“å‡ºæ ¼å¼

### é¢„æµ‹ç»“æœç¤ºä¾‹

```json
{
  "predicted_emotion": "happy",
  "confidence": 0.892,
  "all_emotions": [
    ["happy", 0.892],
    ["neutral", 0.067],
    ["surprise", 0.023],
    ["sad", 0.012],
    ["angry", 0.004],
    ["fear", 0.002]
  ],
  "audio_path": "audio.wav"
}
```

## ğŸ¨ æƒ…ç»ªæ ‡ç­¾è¯´æ˜

| è‹±æ–‡æ ‡ç­¾ | ä¸­æ–‡æ ‡ç­¾ | æè¿° | é¢œè‰² |
|---------|---------|------|------|
| angry | æ„¤æ€’ | ç”Ÿæ°”ã€æ„¤æ€’çš„æƒ…ç»ª | ğŸ”´ çº¢è‰² |
| fear | ææƒ§ | å®³æ€•ã€ææƒ§çš„æƒ…ç»ª | ğŸŸ£ ç´«è‰² |
| happy | å¿«ä¹ | å¼€å¿ƒã€å¿«ä¹çš„æƒ…ç»ª | ğŸŸ¡ é‡‘è‰² |
| sad | æ‚²ä¼¤ | ä¼¤å¿ƒã€æ‚²ä¼¤çš„æƒ…ç»ª | ğŸ”µ è“è‰² |
| surprise | æƒŠè®¶ | æƒŠè®¶ã€æ„å¤–çš„æƒ…ç»ª | ğŸŸ  æ©™è‰² |
| neutral | ä¸­æ€§ | å¹³é™ã€ä¸­æ€§çš„æƒ…ç»ª | âšª ç°è‰² |

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### æé«˜è¯†åˆ«å‡†ç¡®ç‡

1. **éŸ³é¢‘è´¨é‡**: ä½¿ç”¨æ¸…æ™°çš„è¯­éŸ³ï¼Œé¿å…èƒŒæ™¯å™ªéŸ³
2. **æƒ…ç»ªè¡¨è¾¾**: æƒ…ç»ªè¡¨è¾¾è¶Šæ˜æ˜¾ï¼Œè¯†åˆ«å‡†ç¡®ç‡è¶Šé«˜
3. **éŸ³é¢‘é•¿åº¦**: å»ºè®®3-10ç§’çš„è¯­éŸ³ç‰‡æ®µ
4. **è¯­è¨€**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­éŸ³ä¼˜åŒ–ï¼Œæ”¯æŒä¸­æ–‡è¯­éŸ³è¯†åˆ«

### æ€§èƒ½ä¼˜åŒ–

1. **GPUåŠ é€Ÿ**: å¦‚æœæœ‰NVIDIA GPUï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨CUDAåŠ é€Ÿ
2. **æ‰¹é‡å¤„ç†**: ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è¿›è¡Œæ‰¹é‡å¤„ç†
3. **æ¨¡å‹ç¼“å­˜**: é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œåç»­è¿è¡Œä¼šä½¿ç”¨ç¼“å­˜

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥
**é—®é¢˜**: `æ¨¡å‹åŠ è½½å¤±è´¥: ConnectionError`
**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®ä¿å¯ä»¥è®¿é—®huggingface.co

#### 2. CUDAå†…å­˜ä¸è¶³
**é—®é¢˜**: `CUDA out of memory`
**è§£å†³æ–¹æ¡ˆ**: å¼ºåˆ¶ä½¿ç”¨CPUæˆ–å‡å°‘æ‰¹å¤„ç†å¤§å°

#### 3. éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ
**é—®é¢˜**: `éŸ³é¢‘é¢„å¤„ç†é”™è¯¯: Unsupported format`
**è§£å†³æ–¹æ¡ˆ**: è½¬æ¢ä¸ºWAVæ ¼å¼æˆ–ä½¿ç”¨æ”¯æŒçš„éŸ³é¢‘æ ¼å¼

#### 4. ä¾èµ–å®‰è£…å¤±è´¥
**é—®é¢˜**: `pip install å¤±è´¥`
**è§£å†³æ–¹æ¡ˆ**: å‡çº§pipï¼Œå®‰è£…ç¼–è¯‘å·¥å…·ï¼Œæˆ–ä½¿ç”¨condaå®‰è£…

### è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python web_interface.py --debug

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
python -c "
from emotion_speech_recognition import EmotionSpeechRecognition
recognizer = EmotionSpeechRecognition()
print(f'è®¾å¤‡: {recognizer.device}')
print(f'æ¨¡å‹: {recognizer.model}')
"
```

## ğŸ“š APIæ–‡æ¡£

### EmotionSpeechRecognitionç±»

#### ä¸»è¦æ–¹æ³•

##### `predict_emotion(audio_path: str) -> Dict`
é¢„æµ‹éŸ³é¢‘æ–‡ä»¶çš„æƒ…ç»ª

**å‚æ•°:**
- `audio_path` (str): éŸ³é¢‘æ–‡ä»¶è·¯å¾„

**è¿”å›:**
```python
{
    "predicted_emotion": "happy",
    "confidence": 0.892,
    "all_emotions": [["happy", 0.892], ["neutral", 0.067], ...],
    "audio_path": "audio.wav"
}
```

##### `predict_emotion_from_array(audio_array: np.ndarray, sample_rate: int = 16000) -> Dict`
ä»éŸ³é¢‘æ•°ç»„é¢„æµ‹æƒ…ç»ª

**å‚æ•°:**
- `audio_array` (np.ndarray): éŸ³é¢‘æ•°ç»„
- `sample_rate` (int): é‡‡æ ·ç‡ï¼Œé»˜è®¤16000

##### `batch_predict(audio_paths: List[str]) -> List[Dict]`
æ‰¹é‡é¢„æµ‹å¤šä¸ªéŸ³é¢‘æ–‡ä»¶

**å‚æ•°:**
- `audio_paths` (List[str]): éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚

## ğŸ™ è‡´è°¢

- [Hugging Face](https://huggingface.co/) - æä¾›é¢„è®­ç»ƒæ¨¡å‹
- [CASIA](http://www.casia.cn/) - ä¸­æ–‡è¯­éŸ³æ•°æ®é›†
- [Gradio](https://gradio.app/) - Webç•Œé¢æ¡†æ¶
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºå•†ä¸šç”¨é€”ã€‚ 
