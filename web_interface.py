import gradio as gr
import numpy as np
import tempfile
import os
from emotion_speech_recognition import EmotionSpeechRecognition
import matplotlib.pyplot as plt
from io import BytesIO
from PIL import Image
# import seaborn as sns

plt.rcParams['font.sans-serif'] = ['SimHei'] # æ˜¾ç¤ºä¸­æ–‡
plt.rcParams['axes.unicode_minus'] = False # æ˜¾ç¤ºè´Ÿå·

class EmotionRecognitionWebInterface:
    """
    è¯­éŸ³æƒ…ç»ªè¯†åˆ«Webç•Œé¢
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–Webç•Œé¢
        """
        self.emotion_recognizer = None
        self.emotion_colors = {
            "angry": "#FF6B6B",      # çº¢è‰²
            "fear": "#9370DB",       # ç´«è‰²
            "happy": "#FFD700",      # é‡‘è‰²
            "sad": "#4682B4",        # é’¢è“è‰²
            "surprise": "#FF69B4",   # ç²‰çº¢è‰²
            "neutral": "#808080"     # ç°è‰²
        }
        
        # ä¸­æ–‡æƒ…ç»ªæ ‡ç­¾
        self.chinese_emotions = {
            "angry": "æ„¤æ€’",
            "fear": "ææƒ§",
            "happy": "å¿«ä¹",
            "sad": "æ‚²ä¼¤",
            "surprise": "æƒŠè®¶",
            "neutral": "ä¸­æ€§"
        }
    
    def load_model(self):
        """
        åŠ è½½æ¨¡å‹
        """
        if self.emotion_recognizer is None:
            try:
                self.emotion_recognizer = EmotionSpeechRecognition()
                return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼"
            except Exception as e:
                return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        return "âœ… æ¨¡å‹å·²åŠ è½½"
    
    def create_emotion_chart(self, emotion_probs):
        """
        åˆ›å»ºæƒ…ç»ªæ¦‚ç‡å›¾è¡¨
        
        Args:
            emotion_probs: æƒ…ç»ªæ¦‚ç‡åˆ—è¡¨
            
        Returns:
            PIL.Image å¯¹è±¡
        """
        emotions = [item[0] for item in emotion_probs]
        probabilities = [item[1] for item in emotion_probs]
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(10, 6))
        colors = [self.emotion_colors.get(emotion, "#808080") for emotion in emotions]
        
        bars = plt.bar(emotions, probabilities, color=colors, alpha=0.8)
        plt.xlabel('æƒ…ç»ªç±»å‹', fontsize=12)
        plt.ylabel('æ¦‚ç‡', fontsize=12)
        plt.title('è¯­éŸ³æƒ…ç»ªè¯†åˆ«ç»“æœ', fontsize=14, fontweight='bold')
        plt.ylim(0, 1)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, prob in zip(bars, probabilities):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨åˆ°å†…å­˜å¹¶è¿”å›PIL.Imageå¯¹è±¡
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        plt.close()
        buf.seek(0)
        img = Image.open(buf)
        return img
    
    def predict_emotion_from_audio(self, audio_file):
        """
        ä»éŸ³é¢‘æ–‡ä»¶é¢„æµ‹æƒ…ç»ª
        
        Args:
            audio_file: ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        if self.emotion_recognizer is None:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹", None, None
        
        if audio_file is None:
            return "è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", None, None
        
        try:
            # é¢„æµ‹æƒ…ç»ª - audio_file æ˜¯å­—ç¬¦ä¸²è·¯å¾„
            result = self.emotion_recognizer.predict_emotion(audio_file)
            
            if "error" in result:
                return f"âŒ é¢„æµ‹å¤±è´¥: {result['error']}", None, None
            
            # è·å–é¢„æµ‹ç»“æœ
            predicted_emotion = result["predicted_emotion"]
            confidence = result["confidence"]
            all_emotions = result["all_emotions"]
            
            # åˆ›å»ºç»“æœæ–‡æœ¬
            chinese_emotion = self.chinese_emotions.get(predicted_emotion, predicted_emotion)
            result_text = f"""
## ğŸ¯ é¢„æµ‹ç»“æœ

**ä¸»è¦æƒ…ç»ª**: {chinese_emotion} ({predicted_emotion})
**ç½®ä¿¡åº¦**: {confidence:.3f} ({confidence*100:.1f}%)

## ğŸ“Š æ‰€æœ‰æƒ…ç»ªæ¦‚ç‡

"""
            
            for emotion, prob in all_emotions:
                chinese_name = self.chinese_emotions.get(emotion, emotion)
                percentage = prob * 100
                result_text += f"- **{chinese_name}** ({emotion}): {prob:.3f} ({percentage:.1f}%)\n"
            
            # åˆ›å»ºå›¾è¡¨
            chart_path = self.create_emotion_chart(all_emotions)
            
            return result_text, chart_path, audio_file
            
        except Exception as e:
            return f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", None, None
    
    def predict_emotion_from_microphone(self, audio_input):
        """
        ä»éº¦å…‹é£å½•éŸ³é¢„æµ‹æƒ…ç»ª
        
        Args:
            audio_input: Gradio Audioç»„ä»¶è¿”å›çš„å…ƒç»„ (sample_rate, audio_data)
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        if self.emotion_recognizer is None:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹", None, None
        
        if audio_input is None:
            return "è¯·å½•åˆ¶éŸ³é¢‘", None, None
        
        try:
            # è§£åŒ…éŸ³é¢‘æ•°æ® - Gradioè¿”å›çš„æ˜¯ (sample_rate, audio_data)
            sample_rate, audio_data = audio_input
            
            # è°ƒè¯•ä¿¡æ¯
            print(f"éŸ³é¢‘æ•°æ®ç±»å‹: {type(audio_data)}")
            print(f"éŸ³é¢‘æ•°æ®å½¢çŠ¶: {getattr(audio_data, 'shape', 'No shape')}")
            print(f"é‡‡æ ·ç‡: {sample_rate}")
            
            # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯numpyæ•°ç»„
            if not isinstance(audio_data, np.ndarray):
                if isinstance(audio_data, (list, tuple)):
                    audio_data = np.array(audio_data)
                    print(f"è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå½¢çŠ¶: {audio_data.shape}")
                else:
                    print(f"æ— æ³•å¤„ç†çš„æ•°æ®ç±»å‹: {type(audio_data)}")
                    return "âŒ éŸ³é¢‘æ•°æ®æ ¼å¼é”™è¯¯", None, None
            
            # é¢„æµ‹æƒ…ç»ª
            result = self.emotion_recognizer.predict_emotion_from_array(audio_data, sample_rate)
            
            if "error" in result:
                return f"âŒ é¢„æµ‹å¤±è´¥: {result['error']}", None, None
            
            # è·å–é¢„æµ‹ç»“æœ
            predicted_emotion = result["predicted_emotion"]
            confidence = result["confidence"]
            all_emotions = result["all_emotions"]
            
            # åˆ›å»ºç»“æœæ–‡æœ¬
            chinese_emotion = self.chinese_emotions.get(predicted_emotion, predicted_emotion)
            result_text = f"""
## ğŸ¯ é¢„æµ‹ç»“æœ

**ä¸»è¦æƒ…ç»ª**: {chinese_emotion} ({predicted_emotion})
**ç½®ä¿¡åº¦**: {confidence:.3f} ({confidence*100:.1f}%)

## ğŸ“Š æ‰€æœ‰æƒ…ç»ªæ¦‚ç‡

"""
            
            for emotion, prob in all_emotions:
                chinese_name = self.chinese_emotions.get(emotion, emotion)
                percentage = prob * 100
                result_text += f"- **{chinese_name}** ({emotion}): {prob:.3f} ({percentage:.1f}%)\n"
            
            # åˆ›å»ºå›¾è¡¨
            chart_path = self.create_emotion_chart(all_emotions)
            
            return result_text, chart_path, "éº¦å…‹é£å½•éŸ³"
            
        except Exception as e:
            return f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", None, None
    
    def create_interface(self):
        """
        åˆ›å»ºGradioç•Œé¢
        
        Returns:
            Gradioç•Œé¢
        """
        with gr.Blocks(title="è¯­éŸ³æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ") as interface:
            gr.Markdown("""
            # ğŸ¤ è¯­éŸ³æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ
            
            ä½¿ç”¨Hugging Faceä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼Œæ”¯æŒ6ç§æƒ…ç»ªåˆ†ç±»ï¼š
            - ğŸ˜  æ„¤æ€’ (Angry)
            - ğŸ˜¨ ææƒ§ (Fear)
            - ğŸ˜Š å¿«ä¹ (Happy)
            - ğŸ˜¢ æ‚²ä¼¤ (Sad)
            - ğŸ˜² æƒŠè®¶ (Surprise)
            - ğŸ˜ ä¸­æ€§ (Neutral)
            """)
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ¨¡å‹")
                    load_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary")
                    model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æ¨¡å‹æœªåŠ è½½", interactive=False)
                
                with gr.Column():
                    gr.Markdown("### æ¨¡å‹ä¿¡æ¯")
                    gr.Markdown("""
                    - **æ¨¡å‹**: xmj2002/hubert-base-ch-speech-emotion-recognition
                    - **æ”¯æŒæ ¼å¼**: WAV, MP3, M4Aç­‰
                    - **é‡‡æ ·ç‡**: è‡ªåŠ¨é‡é‡‡æ ·åˆ°16kHz
                    - **è®¾å¤‡**: è‡ªåŠ¨æ£€æµ‹CPU/GPU
                    - **è¯­è¨€**: ä¸­æ–‡è¯­éŸ³
                    """)
            
            gr.Markdown("---")
            
            with gr.Tab("ğŸ“ ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"):
                gr.Markdown("### ç¬¬äºŒæ­¥ï¼šä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
                file_input = gr.Audio(
                    label="é€‰æ‹©éŸ³é¢‘æ–‡ä»¶",
                    type="filepath"
                )
                file_btn = gr.Button("ğŸ” åˆ†æéŸ³é¢‘æ–‡ä»¶", variant="primary")
            
            with gr.Tab("ğŸ¤ å®æ—¶å½•éŸ³"):
                gr.Markdown("### ç¬¬äºŒæ­¥ï¼šå½•åˆ¶éŸ³é¢‘")
                mic_input = gr.Audio(
                    label="å½•åˆ¶éŸ³é¢‘",
                    type="numpy"
                )
                mic_btn = gr.Button("ğŸ” åˆ†æå½•éŸ³", variant="primary")
            
            gr.Markdown("---")
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ç¬¬ä¸‰æ­¥ï¼šæŸ¥çœ‹ç»“æœ")
                    result_output = gr.Markdown(label="é¢„æµ‹ç»“æœ")
                    audio_info = gr.Textbox(label="éŸ³é¢‘ä¿¡æ¯", interactive=False)
                
                with gr.Column():
                    gr.Markdown("### ğŸ“Š æƒ…ç»ªæ¦‚ç‡å›¾è¡¨")
                    chart_output = gr.Image(label="æƒ…ç»ªåˆ†å¸ƒå›¾")
            
            # ç»‘å®šäº‹ä»¶
            load_btn.click(
                fn=self.load_model,
                outputs=model_status
            )
            
            file_btn.click(
                fn=self.predict_emotion_from_audio,
                inputs=file_input,
                outputs=[result_output, chart_output, audio_info]
            )
            
            mic_btn.click(
                fn=self.predict_emotion_from_microphone,
                inputs=mic_input,
                outputs=[result_output, chart_output, audio_info]
            )
            
            gr.Markdown("---")
            gr.Markdown("""
            ### ğŸ’¡ ä½¿ç”¨æç¤º
            
            1. **éŸ³é¢‘è´¨é‡**: å»ºè®®ä½¿ç”¨æ¸…æ™°çš„è¯­éŸ³ï¼Œé¿å…èƒŒæ™¯å™ªéŸ³
            2. **éŸ³é¢‘é•¿åº¦**: å»ºè®®3-10ç§’çš„è¯­éŸ³ç‰‡æ®µ
            3. **è¯­è¨€æ”¯æŒ**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­éŸ³ä¼˜åŒ–
            4. **æƒ…ç»ªè¡¨è¾¾**: æƒ…ç»ªè¡¨è¾¾è¶Šæ˜æ˜¾ï¼Œè¯†åˆ«å‡†ç¡®ç‡è¶Šé«˜
            
            ### ğŸ”§ æŠ€æœ¯è¯´æ˜
            
            - ä½¿ç”¨Hubertæ¨¡å‹è¿›è¡Œä¸­æ–‡è¯­éŸ³ç‰¹å¾æå–
            - åŸºäºTransformeræ¶æ„è¿›è¡Œæƒ…ç»ªåˆ†ç±»
            - æ”¯æŒå®æ—¶å¤„ç†å’Œæ‰¹é‡å¤„ç†
            - è‡ªåŠ¨éŸ³é¢‘é¢„å¤„ç†å’Œæ ¼å¼è½¬æ¢
            - åœ¨CASIAä¸­æ–‡è¯­éŸ³æ•°æ®é›†ä¸Šè®­ç»ƒ
            """)
        
        return interface

def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆ›å»ºç•Œé¢
    interface = EmotionRecognitionWebInterface()
    app = interface.create_interface()
    
    # å¯åŠ¨åº”ç”¨
    app.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,  # ç¦ç”¨åˆ†äº«é“¾æ¥
        debug=True
    )

if __name__ == "__main__":
    main() 