# ğŸ“š APIæ–‡æ¡£

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†ä¸­æ–‡è¯­éŸ³æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿçš„APIæ¥å£ï¼ŒåŒ…æ‹¬æ ¸å¿ƒç±»ã€æ–¹æ³•å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

## ç›®å½•

- [EmotionSpeechRecognitionç±»](#emotionspeechrecognitionç±»)
- [EmotionRecognitionWebInterfaceç±»](#emotionrecognitionwebinterfaceç±»)
- [HubertForSpeechClassificationç±»](#hubertforspeechclassificationç±»)
- [HubertClassificationHeadç±»](#hubertclassificationheadç±»)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)

## EmotionSpeechRecognitionç±»

### ç±»æè¿°
è¯­éŸ³æƒ…ç»ªè¯†åˆ«çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£åŠ è½½æ¨¡å‹ã€é¢„å¤„ç†éŸ³é¢‘å’Œè¿›è¡Œæƒ…ç»ªé¢„æµ‹ã€‚

### åˆå§‹åŒ–

```python
EmotionSpeechRecognition(model_name="xmj2002/hubert-base-ch-speech-emotion-recognition")
```

#### å‚æ•°
| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model_name` | str | `"xmj2002/hubert-base-ch-speech-emotion-recognition"` | Hugging Faceæ¨¡å‹åç§° |

#### å±æ€§
| å±æ€§ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `device` | torch.device | è®¡ç®—è®¾å¤‡ï¼ˆCPU/GPUï¼‰ |
| `duration` | int | éŸ³é¢‘å¤„ç†æ—¶é•¿ï¼ˆç§’ï¼‰ |
| `sample_rate` | int | ç›®æ ‡é‡‡æ ·ç‡ |
| `config` | AutoConfig | æ¨¡å‹é…ç½® |
| `processor` | Wav2Vec2FeatureExtractor | ç‰¹å¾æå–å™¨ |
| `model` | HubertForSpeechClassification | æƒ…ç»ªåˆ†ç±»æ¨¡å‹ |
| `emotion_labels` | Dict[int, str] | æƒ…ç»ªæ ‡ç­¾æ˜ å°„ |

### ä¸»è¦æ–¹æ³•

#### `predict_emotion(audio_path: str) -> Dict`

é¢„æµ‹éŸ³é¢‘æ–‡ä»¶çš„æƒ…ç»ªã€‚

**å‚æ•°:**
- `audio_path` (str): éŸ³é¢‘æ–‡ä»¶è·¯å¾„

**è¿”å›:**
```python
{
    "predicted_emotion": "happy",
    "confidence": 0.892,
    "all_emotions": [["happy", 0.892], ["neutral", 0.067], ...],
    "audio_path": "audio.wav"
}
```

**å¼‚å¸¸:**
- `FileNotFoundError`: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨
- `ValueError`: éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ
- `RuntimeError`: æ¨¡å‹é¢„æµ‹å¤±è´¥

**ç¤ºä¾‹:**
```python
recognizer = EmotionSpeechRecognition()
result = recognizer.predict_emotion("audio.wav")
print(f"é¢„æµ‹æƒ…ç»ª: {result['predicted_emotion']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
```

#### `predict_emotion_from_array(audio_array: np.ndarray, sample_rate: int = 16000) -> Dict`

ä»éŸ³é¢‘æ•°ç»„é¢„æµ‹æƒ…ç»ªã€‚

**å‚æ•°:**
- `audio_array` (np.ndarray): éŸ³é¢‘æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (samples,) æˆ– (channels, samples)
- `sample_rate` (int): é‡‡æ ·ç‡ï¼Œé»˜è®¤16000

**è¿”å›:** åŒ `predict_emotion`

**ç¤ºä¾‹:**
```python
import numpy as np
audio_data = np.random.randn(16000)  # 1ç§’éŸ³é¢‘
result = recognizer.predict_emotion_from_array(audio_data, sample_rate=16000)
```

#### `batch_predict(audio_paths: List[str]) -> List[Dict]`

æ‰¹é‡é¢„æµ‹å¤šä¸ªéŸ³é¢‘æ–‡ä»¶çš„æƒ…ç»ªã€‚

**å‚æ•°:**
- `audio_paths` (List[str]): éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨

**è¿”å›:**
```python
[
    {
        "predicted_emotion": "happy",
        "confidence": 0.892,
        "all_emotions": [...],
        "audio_path": "audio1.wav"
    },
    {
        "predicted_emotion": "sad",
        "confidence": 0.756,
        "all_emotions": [...],
        "audio_path": "audio2.wav"
    }
]
```

**ç¤ºä¾‹:**
```python
audio_files = ["audio1.wav", "audio2.wav", "audio3.wav"]
results = recognizer.batch_predict(audio_files)
for result in results:
    print(f"{result['audio_path']}: {result['predicted_emotion']}")
```

#### `preprocess_audio(audio_path: str, target_sr: int = 16000) -> Optional[np.ndarray]`

é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ã€‚

**å‚æ•°:**
- `audio_path` (str): éŸ³é¢‘æ–‡ä»¶è·¯å¾„
- `target_sr` (int): ç›®æ ‡é‡‡æ ·ç‡ï¼Œé»˜è®¤16000

**è¿”å›:**
- `np.ndarray`: é¢„å¤„ç†åçš„éŸ³é¢‘æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (samples,)
- `None`: é¢„å¤„ç†å¤±è´¥

**å¤„ç†æ­¥éª¤:**
1. åŠ è½½éŸ³é¢‘æ–‡ä»¶
2. è½¬æ¢ä¸ºå•å£°é“ï¼ˆå¦‚æœæ˜¯ç«‹ä½“å£°ï¼‰
3. é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
4. è¿”å›å¤„ç†åçš„æ•°ç»„

**ç¤ºä¾‹:**
```python
audio = recognizer.preprocess_audio("audio.wav", target_sr=16000)
if audio is not None:
    print(f"éŸ³é¢‘é•¿åº¦: {len(audio)} é‡‡æ ·ç‚¹")
```

#### `extract_features(audio: np.ndarray) -> Dict[str, torch.Tensor]`

æå–éŸ³é¢‘ç‰¹å¾ã€‚

**å‚æ•°:**
- `audio` (np.ndarray): éŸ³é¢‘æ•°ç»„

**è¿”å›:**
```python
{
    "input_values": torch.Tensor,  # å½¢çŠ¶: (1, max_length)
    "attention_mask": torch.Tensor  # å½¢çŠ¶: (1, max_length)
}
```

**ç¤ºä¾‹:**
```python
audio = recognizer.preprocess_audio("audio.wav")
features = recognizer.extract_features(audio)
print(f"ç‰¹å¾å½¢çŠ¶: {features['input_values'].shape}")
```

#### `id2class(id: int) -> str`

å°†IDè½¬æ¢ä¸ºæƒ…ç»ªç±»åˆ«åç§°ã€‚

**å‚æ•°:**
- `id` (int): æƒ…ç»ªID

**è¿”å›:**
- `str`: æƒ…ç»ªç±»åˆ«åç§°

**æƒ…ç»ªæ˜ å°„:**
```python
{
    0: "angry",      # æ„¤æ€’
    1: "fear",       # ææƒ§
    2: "happy",      # å¿«ä¹
    3: "neutral",    # ä¸­æ€§
    4: "sad",        # æ‚²ä¼¤
    5: "surprise"    # æƒŠè®¶
}
```

## EmotionRecognitionWebInterfaceç±»

### ç±»æè¿°
Webç•Œé¢ç±»ï¼Œæä¾›Gradioç•Œé¢çš„åˆ›å»ºå’Œç®¡ç†ã€‚

### åˆå§‹åŒ–

```python
EmotionRecognitionWebInterface()
```

#### å±æ€§
| å±æ€§ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `emotion_recognizer` | EmotionSpeechRecognition | æƒ…ç»ªè¯†åˆ«å™¨å®ä¾‹ |
| `emotion_colors` | Dict[str, str] | æƒ…ç»ªé¢œè‰²æ˜ å°„ |
| `chinese_emotions` | Dict[str, str] | ä¸­è‹±æ–‡æƒ…ç»ªæ ‡ç­¾æ˜ å°„ |

### ä¸»è¦æ–¹æ³•

#### `load_model() -> str`

åŠ è½½æƒ…ç»ªè¯†åˆ«æ¨¡å‹ã€‚

**è¿”å›:**
- `str`: åŠ è½½çŠ¶æ€æ¶ˆæ¯

**ç¤ºä¾‹:**
```python
interface = EmotionRecognitionWebInterface()
status = interface.load_model()
print(status)  # "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼" æˆ– "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: ..."
```

#### `create_emotion_chart(emotion_probs: List[Tuple[str, float]]) -> PIL.Image`

åˆ›å»ºæƒ…ç»ªæ¦‚ç‡å›¾è¡¨ã€‚

**å‚æ•°:**
- `emotion_probs` (List[Tuple[str, float]]): æƒ…ç»ªæ¦‚ç‡åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [("happy", 0.8), ("sad", 0.2)]

**è¿”å›:**
- `PIL.Image`: æƒ…ç»ªåˆ†å¸ƒå›¾è¡¨

**ç¤ºä¾‹:**
```python
emotions = [("happy", 0.8), ("sad", 0.2)]
chart = interface.create_emotion_chart(emotions)
chart.save("emotion_chart.png")
```

#### `predict_emotion_from_audio(audio_file: str) -> Tuple[str, PIL.Image, str]`

ä»éŸ³é¢‘æ–‡ä»¶é¢„æµ‹æƒ…ç»ªã€‚

**å‚æ•°:**
- `audio_file` (str): éŸ³é¢‘æ–‡ä»¶è·¯å¾„

**è¿”å›:**
- `Tuple[str, PIL.Image, str]`: (ç»“æœæ–‡æœ¬, å›¾è¡¨, éŸ³é¢‘ä¿¡æ¯)

**ç¤ºä¾‹:**
```python
result_text, chart, audio_info = interface.predict_emotion_from_audio("audio.wav")
print(result_text)
```

#### `predict_emotion_from_microphone(audio_input: Tuple[int, np.ndarray]) -> Tuple[str, PIL.Image, str]`

ä»éº¦å…‹é£å½•éŸ³é¢„æµ‹æƒ…ç»ªã€‚

**å‚æ•°:**
- `audio_input` (Tuple[int, np.ndarray]): (é‡‡æ ·ç‡, éŸ³é¢‘æ•°æ®)

**è¿”å›:**
- `Tuple[str, PIL.Image, str]`: (ç»“æœæ–‡æœ¬, å›¾è¡¨, éŸ³é¢‘ä¿¡æ¯)

**ç¤ºä¾‹:**
```python
# Gradio Audioç»„ä»¶è¿”å›çš„æ•°æ®
sample_rate, audio_data = audio_input
result_text, chart, audio_info = interface.predict_emotion_from_microphone((sample_rate, audio_data))
```

#### `create_interface() -> gr.Blocks`

åˆ›å»ºGradioç•Œé¢ã€‚

**è¿”å›:**
- `gr.Blocks`: Gradioç•Œé¢å¯¹è±¡

**ç¤ºä¾‹:**
```python
interface = EmotionRecognitionWebInterface()
app = interface.create_interface()
app.launch(server_name="127.0.0.1", server_port=7860)
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from emotion_speech_recognition import EmotionSpeechRecognition

# åˆå§‹åŒ–æ¨¡å‹
recognizer = EmotionSpeechRecognition()

# é¢„æµ‹å•ä¸ªéŸ³é¢‘æ–‡ä»¶
result = recognizer.predict_emotion("audio.wav")
print(f"é¢„æµ‹æƒ…ç»ª: {result['predicted_emotion']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")

# æ˜¾ç¤ºæ‰€æœ‰æƒ…ç»ªæ¦‚ç‡
for emotion, prob in result['all_emotions']:
    print(f"{emotion}: {prob:.3f}")
```

### æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡é¢„æµ‹å¤šä¸ªæ–‡ä»¶
audio_files = ["audio1.wav", "audio2.wav", "audio3.wav"]
results = recognizer.batch_predict(audio_files)

# ç»Ÿè®¡æƒ…ç»ªåˆ†å¸ƒ
emotion_counts = {}
for result in results:
    if 'error' not in result:
        emotion = result['predicted_emotion']
        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

print("æƒ…ç»ªåˆ†å¸ƒ:")
for emotion, count in emotion_counts.items():
    print(f"{emotion}: {count} æ¬¡")
```

### Webç•Œé¢ä½¿ç”¨

```python
from web_interface import EmotionRecognitionWebInterface

# åˆ›å»ºç•Œé¢
interface = EmotionRecognitionWebInterface()

# åŠ è½½æ¨¡å‹
status = interface.load_model()
print(status)

# é¢„æµ‹éŸ³é¢‘æ–‡ä»¶
result_text, chart, audio_info = interface.predict_emotion_from_audio("audio.wav")
print(result_text)

# å¯åŠ¨Webç•Œé¢
app = interface.create_interface()
app.launch(server_name="127.0.0.1", server_port=7860)
```

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯ç±»å‹

#### 1. æ¨¡å‹åŠ è½½é”™è¯¯
```python
try:
    recognizer = EmotionSpeechRecognition()
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    # æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹åç§°
```

#### 2. éŸ³é¢‘å¤„ç†é”™è¯¯
```python
result = recognizer.predict_emotion("audio.wav")
if "error" in result:
    print(f"å¤„ç†å¤±è´¥: {result['error']}")
    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„
```

#### 3. å†…å­˜ä¸è¶³é”™è¯¯
```python
# å¼ºåˆ¶ä½¿ç”¨CPU
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""

recognizer = EmotionSpeechRecognition()
recognizer.device = torch.device("cpu")
```

### é”™è¯¯æ¢å¤ç­–ç•¥

```python
def safe_predict(recognizer, audio_path):
    """å®‰å…¨çš„é¢„æµ‹å‡½æ•°ï¼ŒåŒ…å«é”™è¯¯å¤„ç†"""
    try:
        result = recognizer.predict_emotion(audio_path)
        if "error" in result:
            return {"error": result["error"], "audio_path": audio_path}
        return result
    except Exception as e:
        return {"error": str(e), "audio_path": audio_path}

# ä½¿ç”¨å®‰å…¨é¢„æµ‹
results = []
for audio_file in audio_files:
    result = safe_predict(recognizer, audio_file)
    results.append(result)

# ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
success_count = len([r for r in results if "error" not in r])
error_count = len([r for r in results if "error" in r])
print(f"æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
```

## æ€§èƒ½ä¼˜åŒ–

### GPUåŠ é€Ÿ
```python
# æ£€æŸ¥GPUå¯ç”¨æ€§
import torch
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("ä½¿ç”¨CPU")
```

### æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
# ä½¿ç”¨æ‰¹å¤„ç†æé«˜æ•ˆç‡
def batch_predict_optimized(recognizer, audio_paths, batch_size=4):
    """ä¼˜åŒ–çš„æ‰¹é‡é¢„æµ‹"""
    results = []
    for i in range(0, len(audio_paths), batch_size):
        batch = audio_paths[i:i+batch_size]
        batch_results = recognizer.batch_predict(batch)
        results.extend(batch_results)
    return results
```

### å†…å­˜ç®¡ç†
```python
# æ¸…ç†GPUå†…å­˜
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with torch.no_grad():
    result = recognizer.predict_emotion("audio.wav")
```

---

**æ³¨æ„**: æœ¬APIæ–‡æ¡£åŸºäºå½“å‰ç‰ˆæœ¬ï¼Œå¦‚æœ‰æ›´æ–°è¯·å‚è€ƒæœ€æ–°ä»£ç ã€‚ 